Time complexity: Amount of time algorithm needs to run relative to input size.
Space complexity: Amount of memory allocated by the algorithm when run relative to the input size.

What we want to focus on is how an algorithm scales with the input size.

When talking about complexity, there are normally three cases:
a. Best case scenario
b. Average case
c. Worst case scenario

It is most correct to use the worst case scenario, but you should be able to talk about the difference between the cases.

Time complexity example:
// Given an integer array "arr" with length n,
for (int num: arr) {
    print(num)
}
It has time complexity O(n)
-> In each for loop iteration, we are performing print which costs O(1)
-> For loop iterates n times, which gives time-complexity O(1.n) = O(n)

eg2:
for (int num: arr) {  ----> Iterates n times
    for (int i = 0; i < 500,000; i++) {  -> Iterates 500,000 times
        print(num)                       -> Time complexity: O(1)
    }
}
Total Time complexity: O(500,000*n) = O(n)

eg3:
for (int num: arr) {        -> O(n)
    for (int num2: arr) {   -> O(n)
        print(num * num2)   -> Time complexity: O(1)
    }
}
Total Time complexity: O(n*n) = O(n^2)

eg4:
// Given integer arrays "arr" with length n and "arr2" with length m,

for (int num: arr) {    -> O(n)
    print(num)
}

for (int num: arr) {    -> O(n)
    print(num)
}

for (int num: arr2) {    ->O(m)
    print(num)
}

Total time complexity: O(n+n+m) = O(2n+m) = O(n+m)

eg5:
// Given an integer array "arr" with length n,

for (int i = 0; i < arr.length; i++) {
    for (int j = i; j < arr.length; j++) {
        print(arr[i] + arr[j])
    }
}

The inner for loop is dependent on what iteration the outer for loop is currently on.
First time, inner for loop runs n times.
Second time, inner for loop runs n-1 times.
Third time, inner for loop runs n-2 times and so on.

Total iterations = 1+2+..+n = n(n+1)/2 = O(n^2)

Note: Time complexity for efficient sorting algorithms: O(nlogn)
Note: Base of the logarithm doesn't actually matter for big O, since all logarithms are related by a constant factor.
eg: logN of base b = logN/logb = O(logN)

                    Space Complexity
-> Never count space used by input unless asked by interviewer.
-> Never count space used by output unless asked by interviewer.

-> Count only memory what code allocates
eg:
// Given an integer array "arr" with length n

for (int num: arr) {
    print(num)
}
Space complexity: O(1), only space allocated is num which is constant relative to n.

eg2:
// Given an integer array "arr" with length n

Array doubledNums = int[]

for (int num: arr) {
    doubledNums.add(num * 2)
}

Space complexity: O(n). Array 'doubledNums' store n integers at end of algorithm.

eg3:
// Given an integer array "arr" with length n

Array nums = int[]
int oneHundredth = n / 100

for (int i = 0; i < oneHundredth; i++) {
    nums.add(arr[i])
}
Space complexity: nums[] store 1% of numbers in arr[]. So, O(n/100) = O(n)

eg4:
// Given integer arrays "arr" with length n and "arr2" with length m,

Array grid = int[n][m]

for (int i = 0; i < arr.length; i++) {
    for (int j = 0; j < arr2.length; j++) {
        grid[i][j] = arr[i] * arr2[j]
    }
}

Space complexity: O(n.m) We are creating grid that has dimensions n.m

                                        Recursion
Any iterative algorithm can be written recursively.
Note: We need base condition for every recursive function to stop.
An important thing to understand about recursion is the order in which the code runs - the order in which the computer executes instructions.

eg: Evaluate the flow of execution for
function fn(i):
  if i > 3:
    return

  print(i)
  fn(i + 1)
  print(f"End of call where i = {i}")
  return

fn(1)

Where recursion shines is when you use it to break down a problem into "subproblems", whose solutions can then be combined to solve the original problem.
eg: Fibonacci numbers
Fn = Fn-1 + Fn-2

function F(n):
    if n <= 1:
        return n

    oneBack = F(n - 1)
    twoBack = F(n - 2)
    return oneBack + twoBack

F(0) = 0, F(1) = 1

eg: Find F(3)

oneBack = F(2)
    oneBack = F(1)
        F(1) = 1
    twoBack = F(0)
        F(0) = 0
    F(2) = oneBack + twoBack = 1
twoBack = F(1)
    F(1) = 1
F(3) = oneBack + twoBack = 2

F(3) can be broken down into 2 sub-problems: F(2) and F(1). Combinining solutions of F(2) and F(1) we can get F(3).
By determining the base cases and a recurrence relation, we can easily implement the function

